# ğŸ¤– Guia para Treinar Seu PrÃ³prio Modelo de IA

## VisÃ£o Geral

Este guia te ajudarÃ¡ a comeÃ§ar a treinar seu prÃ³prio modelo de IA baseado no GLM, para que vocÃª possa ter o ZEX funcionando sem depender de serviÃ§os pagos.

## ğŸ“‹ PrÃ©-requisitos

### Hardware NecessÃ¡rio
- **GPU NVIDIA** com pelo menos 8GB VRAM (recomendado: 16GB+)
- **RAM**: MÃ­nimo 16GB (recomendado: 32GB+)
- **Armazenamento**: Pelo menos 100GB livres (SSD recomendado)
- **CPU**: Processador moderno (Intel i7/AMD Ryzen 7 ou superior)

### Software NecessÃ¡rio
- **Python 3.8+**
- **CUDA** (para GPU NVIDIA)
- **Git**
- **Docker** (opcional, mas recomendado)

## ğŸš€ OpÃ§Ãµes de Treinamento

### OpÃ§Ã£o 1: Fine-tuning do GLM (Recomendado para Iniciantes)

#### Passo 1: Preparar Ambiente

```bash
# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate peft
pip install jupyter notebook
```

#### Passo 2: Baixar Modelo Base GLM

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Baixar modelo GLM-4 ou similar
model_name = "THUDM/glm-4-9b-chat"  # ou outro modelo GLM disponÃ­vel
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
```

#### Passo 3: Preparar Dados de Treinamento

Crie um arquivo `training_data.json` com conversas do ZEX:

```json
[
    {
        "instruction": "VocÃª Ã© o ZEX, um assistente de IA.",
        "input": "OlÃ¡, como vocÃª estÃ¡?",
        "output": "OlÃ¡! Estou funcionando perfeitamente. Como posso ajudar vocÃª hoje?"
    },
    {
        "instruction": "VocÃª Ã© o ZEX, um assistente de IA.",
        "input": "Que dia Ã© hoje?",
        "output": "Hoje Ã© [data atual]. Como posso ajudar?"
    }
]
```

#### Passo 4: Script de Fine-tuning

Crie `train_zex.py`:

```python
from transformers import Trainer, TrainingArguments
from datasets import Dataset
import json

# Carregar dados
with open('training_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

dataset = Dataset.from_list(data)

# Configurar treinamento
training_args = TrainingArguments(
    output_dir="./zex_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=True,  # Usar precisÃ£o mista
    learning_rate=2e-5,
)

# Criar trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

# Treinar
trainer.train()
trainer.save_model("./zex_model_final")
```

### OpÃ§Ã£o 2: Usar LoRA (Low-Rank Adaptation) - Mais Eficiente

LoRA permite treinar com menos recursos:

```python
from peft import LoraConfig, get_peft_model, TaskType

# Configurar LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,  # Rank
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["query_key_value", "dense"]
)

# Aplicar LoRA ao modelo
model = get_peft_model(model, lora_config)
```

### OpÃ§Ã£o 3: Treinamento em Cloud (Sem GPU Local)

#### Google Colab (Gratuito com limitaÃ§Ãµes)
- Acesso a GPU T4 (gratuito, mas com timeout)
- GPU A100 (pago)

#### Kaggle (Gratuito)
- GPU P100 (30h/semana grÃ¡tis)

#### RunPod / Vast.ai (Pago, mas barato)
- Aluguel de GPU por hora
- A partir de $0.20/hora

## ğŸ“Š Coletar Dados de Treinamento

### EstratÃ©gias para Dados

1. **Conversas do ZEX Atual**
   - Exportar histÃ³rico de conversas
   - Limpar e formatar dados
   - Criar pares pergunta-resposta

2. **Dados PÃºblicos**
   - PortuguÃªs brasileiro
   - Conversas naturais
   - Perguntas e respostas

3. **Gerar Dados SintÃ©ticos**
   - Usar o prÃ³prio ZEX para gerar exemplos
   - Diversificar perguntas e respostas

### Script para Exportar Conversas

Adicione ao `script.js`:

```javascript
function exportConversations() {
    const conversations = JSON.parse(localStorage.getItem('conversationHistory') || '[]');
    const dataStr = JSON.stringify(conversations, null, 2);
    const blob = new Blob([dataStr], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'zex_conversations.json';
    a.click();
}
```

## ğŸ¯ Estrutura de Projeto Recomendada

```
zex-training/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Dados brutos
â”‚   â”œâ”€â”€ processed/        # Dados processados
â”‚   â””â”€â”€ training_data.json
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/             # Modelo base GLM
â”‚   â””â”€â”€ zex/              # Modelo treinado
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py   # Preparar dados
â”‚   â”œâ”€â”€ train.py          # Treinar modelo
â”‚   â””â”€â”€ evaluate.py       # Avaliar modelo
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb # AnÃ¡lise exploratÃ³ria
â””â”€â”€ requirements.txt
```

## ğŸ”§ IntegraÃ§Ã£o com o ZEX

### OpÃ§Ã£o A: API Local

Criar servidor Flask/FastAPI:

```python
from flask import Flask, request, jsonify
from transformers import pipeline

app = Flask(__name__)
model = pipeline("text-generation", model="./zex_model_final")

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    message = data.get('message', '')
    response = model(message, max_length=200, temperature=0.7)
    return jsonify({'response': response[0]['generated_text']})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### OpÃ§Ã£o B: IntegraÃ§Ã£o Direta (Ollama)

Usar Ollama para rodar modelos localmente:

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo GLM (se disponÃ­vel) ou usar Llama
ollama pull llama2

# Criar modelo customizado
ollama create zex -f Modelfile
```

## ğŸ“ˆ MÃ©tricas de AvaliaÃ§Ã£o

- **Perplexidade**: QuÃ£o bem o modelo prevÃª o prÃ³ximo token
- **BLEU Score**: Similaridade com respostas de referÃªncia
- **AvaliaÃ§Ã£o Humana**: Testes com usuÃ¡rios reais

## ğŸ’° Custos Estimados

### Treinamento Local
- **Eletricidade**: ~$50-100/mÃªs (dependendo da regiÃ£o)
- **Hardware**: Investimento inicial $1000-3000

### Treinamento em Cloud
- **Google Colab Pro**: $10/mÃªs
- **RunPod**: ~$50-200 (dependendo do tempo de treinamento)
- **AWS/GCP**: $100-500 (dependendo da configuraÃ§Ã£o)

## ğŸš§ Desafios e SoluÃ§Ãµes

### Desafio 1: Falta de GPU
**SoluÃ§Ã£o**: Usar LoRA ou treinar em cloud

### Desafio 2: Poucos Dados
**SoluÃ§Ã£o**: 
- Usar dados pÃºblicos
- Gerar dados sintÃ©ticos
- Fine-tuning com poucos exemplos (few-shot)

### Desafio 3: Qualidade do Modelo
**SoluÃ§Ã£o**: 
- Iterar no treinamento
- Ajustar hiperparÃ¢metros
- Coletar mais dados de qualidade

## ğŸ“š Recursos Ãšteis

- **Hugging Face**: https://huggingface.co
- **Papers with Code**: https://paperswithcode.com
- **GLM GitHub**: https://github.com/THUDM/GLM
- **Transformers Docs**: https://huggingface.co/docs/transformers

## ğŸ“ PrÃ³ximos Passos

1. **ComeÃ§ar Pequeno**: Treine com poucos dados primeiro
2. **Iterar**: Melhore gradualmente
3. **Avaliar**: Teste com usuÃ¡rios reais
4. **Expandir**: Adicione mais dados e recursos

## âš ï¸ ConsideraÃ§Ãµes Importantes

- **Tempo**: Treinamento pode levar horas ou dias
- **Recursos**: Requer hardware adequado
- **ManutenÃ§Ã£o**: Modelos precisam de atualizaÃ§Ã£o periÃ³dica
- **Custos**: Mesmo local, hÃ¡ custos de energia

## ğŸ”„ Alternativa: Modelos Open Source

Se treinar for muito complexo, considere:

- **Llama 2** (Meta) - Open source, pode rodar localmente
- **Mistral** - Modelo open source eficiente
- **Ollama** - Facilita rodar modelos localmente
- **LM Studio** - Interface grÃ¡fica para modelos locais

Esses podem ser uma ponte atÃ© vocÃª ter seu modelo prÃ³prio treinado.

